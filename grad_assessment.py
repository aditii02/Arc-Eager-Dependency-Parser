# -*- coding: utf-8 -*-
"""grad_assessment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KHUM8V-JtRdxQJNsBr6XUHu9yr43xEQd
"""

!wget "https://www.csee.umbc.edu/courses/undergraduate/473/f22/materials/a1/data/UD_English-EWT/en_ewt-ud-train.conllu"

!pip install conllu
import conllu as con
from itertools import chain
import torch
import torch.nn as nn

with open('en_ewt-ud-train.conllu') as train_file:
  train_data = [train for train in con.parse_incr(train_file)]

upos_data=[]
deprel_data=[]
form_data=[]

for word in train_data:
  for val in word:
    if (val['form']) not in form_data:
      form_data.append(val['form'])
    if (val['upos']) not in upos_data:
      upos_data.append(val['upos']) 
    if (val['deprel']) not in deprel_data:
      deprel_data.append(val['deprel'])  

upos_dict={}
form_dict={}
deprel_dict={}
counter=1
for f in form_data:
  form_dict[f] = counter
  counter+=1

counter=1
for u in upos_data:
  upos_dict[u]=counter
  counter+=1
counter=1  
for d in deprel_data:
  deprel_dict[d]=counter
  counter+=1

# creating word tensors embeddings which will be refered by the model 

embeds_upos = nn.Embedding(len(upos_dict)+1,10)  
embeds_form = nn.Embedding(len(form_dict)+1,50)
embeds_deprel = nn.Embedding(len(deprel_data)+1,10)

upos_tensor_embed=[]
form_tensor_embed=[]
deprel_tensor_embed=[]

for key,val in upos_dict.items():
  lookup_tensor = torch.tensor((val),dtype=torch.long)  
  upos_tensor_embed.append(embeds_upos(lookup_tensor))
for key,val in form_dict.items():
  lookup_tensor=torch.tensor((val),dtype=torch.long)  
  form_tensor_embed.append(embeds_form(lookup_tensor))
for key,val in deprel_dict.items():
  lookup_tensor=torch.tensor((val),dtype=torch.long) 
  deprel_tensor_embed.append(embeds_deprel(lookup_tensor))

import sys

# this class performs transition after applying model.predict which predicts the next transition based on the current state of buffer, stack and return a list of dependencies 

class Sentence_Parse(object):
    def __init__(self, sentence):

        self.stack = ["ROOT"]
        self.buffer = sentence[:]
        self.dependencies = []

        ### END YOUR CODE


    def parse_step(self, transition_action):
      # Performs a transition for the current state , which could be either "Shift", "Left Arc" or "Right Arc"
        
        if self.buffer and transition_action == "shift":
            self.stack.append(self.buffer.pop(0))
        elif len(self.stack) >= 2 and transition_action == "LeftArc":
            self.dependencies.append((self.stack[-1], self.stack[-2]))
            self.stack.pop(-2)
        elif len(self.stack) >= 2 and transition_action == "RightArc":
            self.dependencies.append((self.stack[-2], self.stack[-1]))
            self.stack.pop(-1)
        ### END YOUR CODE

    def call_parse_step(self, transitions):
        # for every transition state apply parse_sentence
        for tr in transitions:
            self.parse_step(tr)
        return self.dependencies


def batch_parse(sentences, parse_model, batch_size):
    #Parses a list of sentences in minibatches using a model.
    dependencies = []

    #assert batch_size != 0

    stparse_objects = [Sentence_Parse(s) for s in sentences]
    parses = stparse_objects

    while parses:
        sentence_batch = parses[:batch_size]
        while sentence_batch:
            transitions = parse_model.predict(sentence_batch)
            # print(transitions)
            for parse_obj,transition in zip(sentence_batch,transitions):
                parse_obj.call_parse_step(transition)
            sentence_batch = [parser for pr in sentence_batch if len(parse_obj.stack) > 1 or parse_obj.buffer]
            # print(len(batch_parser))
        sentence_batch = sentence_batch[batch_size:]
    
    dependencies = [parse_obj.dependencies for pr in stparse_objects]

    return dependencies

# implementing the model

import time
import pickle
import torch.nn.functional as F
import os
import torch
import torch.nn as nn


# intiliazing model with the embeddings size , output vector probability class , hidden layer size and weights

