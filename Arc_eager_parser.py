# -*- coding: utf-8 -*-


Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KHUM8V-JtRdxQJNsBr6XUHu9yr43xEQd
"""
# import the UD_English-EWT/en-ewt_ud_train.conllu dataset

!pip install conllu
import conllu as con
from itertools import chain
import torch
import torch.nn as nn
import time
import pickle
import torch.nn.functional as F
import os
import torch
import torch.nn as nn


with open('en_ewt-ud-train.conllu') as train_file:
  train_data = [train for train in con.parse_incr(train_file)]

upos_data=[]
deprel_data=[]
form_data=[]

for word in train_data:
  for val in word:
    if (val['form']) not in form_data:
      form_data.append(val['form'])
    if (val['upos']) not in upos_data:
      upos_data.append(val['upos']) 
    if (val['deprel']) not in deprel_data:
      deprel_data.append(val['deprel'])  

upos_dict={}
form_dict={}
deprel_dict={}
counter=1
for f in form_data:
  form_dict[f] = counter
  counter+=1

counter=1
for u in upos_data:
  upos_dict[u]=counter
  counter+=1
counter=1  
for d in deprel_data:
  deprel_dict[d]=counter
  counter+=1

# creating word tensors embeddings which will be refered by the model 

embeds_upos = nn.Embedding(len(upos_dict)+1,10)  
embeds_form = nn.Embedding(len(form_dict)+1,50)
embeds_deprel = nn.Embedding(len(deprel_data)+1,10)

upos_tensor_embed=[]
form_tensor_embed=[]
deprel_tensor_embed=[]

for key,val in upos_dict.items():
  lookup_tensor = torch.tensor((val),dtype=torch.long)  
  upos_tensor_embed.append(embeds_upos(lookup_tensor))
for key,val in form_dict.items():
  lookup_tensor=torch.tensor((val),dtype=torch.long)  
  form_tensor_embed.append(embeds_form(lookup_tensor))
for key,val in deprel_dict.items():
  lookup_tensor=torch.tensor((val),dtype=torch.long) 
  deprel_tensor_embed.append(embeds_deprel(lookup_tensor))

import sys

# this class performs transition after applying model.predict which predicts the next transition based on the current state of buffer, stack and return a list of dependencies 

class Sentence_Parse(object):
    def __init__(self, sentence):

        self.stack = ["ROOT"]
        self.buffer = sentence[:]
        self.dependencies = []

        ### END YOUR CODE


    def parse_step(self, transition_action):
      # Performs a transition for the current state , which could be either "Shift", "Left Arc" or "Right Arc"
        
        if self.buffer and transition_action == "shift":
            self.stack.append(self.buffer.pop(0))
        elif len(self.stack) >= 2 and transition_action == "LeftArc":
            self.dependencies.append((self.stack[-1], self.stack[-2]))
            self.stack.pop(-2)
        elif len(self.stack) >= 2 and transition_action == "RightArc":
            self.dependencies.append((self.stack[-2], self.stack[-1]))
            self.stack.pop(-1)
        ### END YOUR CODE

    def call_parse_step(self, transitions):
        # for every transition state apply parse_sentence
        for tr in transitions:
            self.parse_step(tr)
        return self.dependencies


def batch_parse(sentences, parse_model, batch_size):
    #Parses a list of sentences in minibatches using a model.
    dependencies = []

    #assert batch_size != 0

    stparse_objects = [Sentence_Parse(s) for s in sentences]
    parses = stparse_objects

    while parses:
        sentence_batch = parses[:batch_size]
        while sentence_batch:
            transitions = parse_model.predict(sentence_batch)
            # print(transitions)
            for parse_obj,transition in zip(sentence_batch,transitions):
                parse_obj.call_parse_step(transition)
            sentence_batch = [parser for pr in sentence_batch if len(parse_obj.stack) > 1 or parse_obj.buffer]
            # print(len(batch_parser))
        sentence_batch = sentence_batch[batch_size:]
    
    dependencies = [parse_obj.dependencies for pr in stparse_objects]

    return dependencies

# implementing the model
# intiliazing model with the embeddings size , output vector probability class , hidden layer size and weights

class ParserModel(nn.Module):
    
    def __init__(self, feature_embeddings, feature_vector=50,
        hiddenlayer_size=50, output_class=3, dropout_prob=0.5):
        super(ParserModel, self).__init__()
        self.dropout_prob = dropout_prob
        self.output_class = output_class
        self.embedding_size = feature_embeddings.shape[1]
        self.feature_vector = feature_vector
        self.hiddenlayer_size = hiddenlayer_size
        self.embeddings_weights = nn.Parameter(torch.tensor(feature_embeddings))
        self.hiddenlayer = nn.Linear(self.embedding_size * self.feature_vector, self.hiddenlayer_size) # create a hidden layer with size embeddings_size * feature vector and output is a vector of a specific size defined
        self.map_logits = nn.Linear(self.hiddenlayer_size,self.output_class) # hidden layer computed above is passed through the weights and an output vector of size 3 is generated
   
   
    def get_embeddings(self, input):

      print("get embeddings")
      
      # will return the embeddings of the feature vector 


    def forward(self, input_vector):
      # to run the model forward 
      final_embeddings = self.get_embeddings(input_vector)
      final_embeddings= self.hiddenlayer(final_embeddings)
      #final_embeddings = self.map_logits(self.hiddenlayer_size,self.output_class )
      final_embeddings = nn.functional.relu(final_embeddings)
      # final_embeddings = self.dropout(final_embeddings) #dropout to be decided
      output_logits = self.map_logits(final_embeddings)

      return output_logits       
    
    
    
    
    







