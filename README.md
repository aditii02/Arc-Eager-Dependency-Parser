# Arc-Eager-Dependency-Parser

Neural Arc Dependency Parser 

Dependency Structure shows which words are depended on the which other words i.e., how a word modifies, attaches or is an argument of another word. We can represent this with the help of asymmetric
relations(arrows) called dependencies.
The arrows generally depict which type of grammatical relations like subject, prepositional object,apposition exist between two words. The arrows show the connection between head and a dependent and
forms a dependency tree. For example, consider a sentence: Book me the morning flight. The relation between “Book” and “me”
is defined as “iobj” where “me” is dependent on the word “book”, therefore the arc can be drawn as
RightArc. Similarly, “morning” is dependent on “flight” and a LeftArc can be drawn between both the words from head(flight) to dependent(the) root iobj (RA) nmod(LA)

Implementation Route:

The Project involves working on a Transition based Dependency Parser on Universal Dependency EWT – English dataset.The Project implements an Arc Eager Parser, and the parsing process is modeled as a sequence of
transitions to form a dependency tree which clearly depicts the words and their dependencies. At each step of the algorithm a current state is maintained which represents A stack – words currently being
processed, A buffer – words yet to be processed. A list of stack states is generated with their corresponding label (“S”,”LA”,”RA”) which resulted from any transition that took place at that time of
the stack.Initially the buffer contains all the words of a sentence as a list of string, the stack contains only the “root”
and the dependency list is empty. At each step the parser applies a transition based on the rules defined in the parsing oracle(figure 1). The
three transitions that can be applied are:
1. SHIFT - Removes the first word from the buffer and append it in the stack.
2. RIGHT-ARC – It marks the first added item to the stack as the dependent of second item and removes the first item from the buffer and appends to the stack.
3. LEFT-ARC – It marks the second added item to the stack as a dependent of first item and removes the second item from the stack.
As described in the above figure, when a batch of sentences is passed through this parser it generates states and labels for each sentence which is the training data for our model .
For the EWT dataset the number of stack states generated is : 194525 (figure 2)
And the no of labels = 194525 For every sentence, a certain number of stack states have been generated by the parser and in each state
there are certain number of words. For every word , its “upos” label and “deprel” has been attached to the word in a form of a string which will be fed to the model .
For example: if the state of the stack contains two words [”the”,”cat”] Then the string formed is of the format : words + uposlabels + deprel = “the cat Article Noun iobjnmod”.

Working of Neural Network Classifier:
The model takes input as generated by the parser which of the size 194525 in EWT dataset , the neural network classifier consists of three layers , the first is an embeddings layer which is used to embed each string as generated in (1) and convert it into a size of 500 vector representation , the second layer consists
of LSTM which has 500 nodes and activation function = “sigmoid” to compute the weights with a dropout of 0.2 . The third layer is a dense layer which consists of 64 nodes with activation function =
“SoftMax”.

The last layer is a dense layer which generates the final output which could be either “S”,”RA” or “LA” for a particular state of stack.
The model is optimized with the help of SGD with the learning rate of 0.0001 and the weights are adjusted to reduce the Sparse categorical cross entropy loss (figure 3) .
The Accuracy reached by the model is around 57.77 % over 10 epochs with batch size =50, which was pretty descent. 


References :
1. https://nlp.stanford.edu/pubs/emnlp2014-depparser.pdf
2. https://arxiv.org/pdf/1901.10457.pdf
3. https://pytorch.org/docs/stable/nn.html
4. https://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture04-dep-parsing.pdf
5. https://keras.io/api/optimizers/sgd/
